{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980M (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "\n",
    "# Required libraries\n",
    "import h5py\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('/work/code/repo/d-script/')\n",
    "# d-script imports\n",
    "from data_iters.minibatcher import MiniBatcher\n",
    "from data_iters.iam_hdf5_iterator import IAM_MiniBatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_file = h5py.File('../../../../docingest/lines.hdf5','r')\n",
    "author_hdf5_file = h5py.File('../../../../docingest/author_lines.hdf5','r')\n",
    "file_list = hdf5_file.keys()\n",
    "num_authors=47\n",
    "num_forms_per_author=500\n",
    "shingle_dim=(120,120)\n",
    "use_form=True\n",
    "\n",
    "batch_size=32\n",
    "lr = 0.01\n",
    "total_iters=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model\n",
      "Finished compilation\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "def randangle(batch):\n",
    "    newbatch = np.zeros(batch.shape)\n",
    "    for i,im in enumerate(batch):\n",
    "        imangle = np.asarray(Image.fromarray(im.squeeze()).rotate(7.5*np.random.randn()))\n",
    "        newbatch[i]=imangle\n",
    "    return newbatch\n",
    "\n",
    "# Get a shingle from a line with dimension shingle_dim\n",
    "def get_shingle(original_line, shingle_dim):\n",
    "    # Pull shingle from the line\n",
    "    (height, width) = original_line.shape\n",
    "    max_x = max(width - shingle_dim[1], 0)\n",
    "    max_y = max(height - shingle_dim[0], 0)\n",
    "    x_start = random.randint(0, max_x)\n",
    "    y_start = random.randint(0, max_y)\n",
    "    if width < shingle_dim[1] or height < shingle_dim[0]: # The line is too small in at least one access\n",
    "        output_arr = np.zeros(shingle_dim)\n",
    "        output_arr.fill(255)\n",
    "        output_arr[:height,:width] = original_line[:min(height, shingle_dim[0]), :min(width, shingle_dim[1])]\n",
    "        return output_arr\n",
    "    else:\n",
    "        return original_line[y_start:y_start+ shingle_dim[0], x_start:x_start+shingle_dim[1]]\n",
    "\n",
    "# From a dictionary, get a random sample\n",
    "def sample_dictionary( the_dict ):\n",
    "    keys = the_dict.keys()\n",
    "    sampleno = np.random.randint(0, len(keys))\n",
    "    randkey = keys[sampleno]\n",
    "    return the_dict[ randkey ]    \n",
    "    \n",
    "# From an HDF5 file, a list of author ID's, return a minibatch\n",
    "def get_batch( author_hdf5_file, author_ids, shingle_size=(120,120), data_size=32 ):\n",
    "    \n",
    "    author_keys = author_ids.keys()\n",
    "    author_rands = np.random.randint(0, len(author_keys), data_size)\n",
    "\n",
    "    author_batch = np.zeros( (data_size, shingle_size[0], shingle_size[1]))\n",
    "    author_truth = np.zeros( data_size )\n",
    "    for i, author_rand in enumerate(author_rands):\n",
    "        author_group = author_hdf5_file[ author_keys[author_rand] ]\n",
    "        author_batch[i,:,:] = get_shingle( sample_dictionary( author_group ).value , shingle_size)\n",
    "        author_truth[i] = author_ids[ author_keys[author_rand] ]\n",
    "        \n",
    "    return author_batch, author_truth\n",
    "\n",
    "if True:\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(48, 12, 12,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=(1, shingle_dim[0], shingle_dim[1])))\n",
    "\n",
    "    model.add(BN())\n",
    "    #model.add(PReLU())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(48, 6, 6))\n",
    "    model.add(BN())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(PReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution2D(128, 6, 6, border_mode = 'valid'))\n",
    "    model.add(BN())\n",
    "    model.add(Activation('relu'))\n",
    "    #    model.add(PReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #    #model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode = 'valid'))\n",
    "    model.add(BN())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(PReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BN())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(128))\n",
    "    model.add(BN())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_authors))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    print \"Compiling model\"\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.7, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "    print \"Finished compilation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('fielnet.hdf5')\n",
    "author_ids = pickle.load(open('fielauthors.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "320/320 [==============================] - 3s - loss: 0.6682 - acc: 0.7906     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c3be67050>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_s, Y_s) = get_batch(author_hdf5_file, author_ids, data_size=32*10)\n",
    "X_s = 1.0 - X_s / 255.0\n",
    "X_s = np.expand_dims(X_s, 1)\n",
    "X_s = randangle(X_s)\n",
    "Y_s = to_categorical(Y_s, num_authors)\n",
    "model.fit(X_s, Y_s, batch_size=batch_size, nb_epoch=1, show_accuracy=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
