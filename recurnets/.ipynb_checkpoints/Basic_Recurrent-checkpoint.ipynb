{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Recurrent Neural Network\n",
    "\n",
    "Testing out original code for a simple LSTM to understand the sequential writing of an author from left to right. (To do: bi-directional recurrent LSTMs.)\n",
    "\n",
    "Details: \n",
    "We require two additional layers that I've written to make the dimensions of the input to other layers consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "import keras\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "from keras.layers.core import Layer\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "import theano.tensor as T\n",
    "\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from minibatcher import MiniBatcher\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Keras layers for use in the recurrent network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Squeeze(Layer):\n",
    "    '''\n",
    "        Get rid of any dimensions of size 1.\n",
    "        First dimension is assumed to be nb_samples.\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Squeeze, self).__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        input_shape = self.input_shape\n",
    "        data_shape = tuple( np.array(input_shape)[ np.array(input_shape) > 1 ] )\n",
    "        return (input_shape[0],)+ data_shape\n",
    "\n",
    "    def get_output(self, train=False):\n",
    "        X = self.get_input(train)\n",
    "        # size = T.prod(X.shape) // X.shape[0]\n",
    "        # nshape = (X.shape[0], size)\n",
    "        # return T.reshape(X, output_shape)\n",
    "        return X.squeeze()\n",
    "    \n",
    "class Transpose3(Layer):\n",
    "    '''\n",
    "        Get rid of any dimensions of size 1.\n",
    "        First dimension is assumed to be nb_samples.\n",
    "    '''\n",
    "    def __init__(self, transpose_order, **kwargs):\n",
    "        self.transpose_order = transpose_order\n",
    "        super(Transpose3, self).__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        input_shape = self.input_shape\n",
    "        data_shape = ()\n",
    "        for j in self.transpose_order:\n",
    "            data_shape+=(input_shape[j],)\n",
    "        return data_shape\n",
    "\n",
    "    def get_output(self, train=False):\n",
    "        X = self.get_input(train)\n",
    "        # size = T.prod(X.shape) // X.shape[0]\n",
    "        # nshape = (X.shape[0], size)\n",
    "        # return T.reshape(X, output_shape)\n",
    "        return X.transpose(self.transpose_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data (40 authors, 15 forms per author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_authors=40\n",
    "num_forms_per_author=15\n",
    "\n",
    "\n",
    "hdf5_file = '/memory/author_lines.hdf5'\n",
    "\n",
    "fIn = h5py.File(hdf5_file, 'r')\n",
    "authors = []\n",
    "\n",
    "# Filter on number of forms per author\n",
    "for author in fIn.keys():\n",
    "    if len(fIn[author]) > num_forms_per_author:\n",
    "        authors.append(author)\n",
    "\n",
    "if len(authors) < num_authors:\n",
    "    raise ValueError(\"There are only %d authors with more than %d forms\"%(len(authors), num_forms_per_author))\n",
    "keys = []\n",
    "# Get all the keys from our hdf5 file\n",
    "for author in authors[:num_authors]: # Limit us to num_authors\n",
    "    forms = list(fIn[author])\n",
    "    for form in forms[:num_forms_per_author]: # Limit us to num_form_per_author\n",
    "        for line_name in fIn[author][form].keys():\n",
    "            for shingle in range(fIn[author][form][line_name].shape[0]):\n",
    "                keys.append([(author,form,line_name), shingle])\n",
    "\n",
    "# Normalization function which scales values from 0 (white) to 1 (black)\n",
    "normalize = lambda x: 1.0 - x.astype(np.float32)/255.0\n",
    "\n",
    "m = MiniBatcher(fIn, keys,normalize=normalize, batch_size=32, min_shingles=20*7*num_forms_per_author)\n",
    "\n",
    "m.batch_size = 32*20\n",
    "m.set_mode(MiniBatcher.TEST)\n",
    "[X_test, Y_test] = m.get_batch()\n",
    "X_test = np.expand_dims(X_test, 1)\n",
    "Y_test = to_categorical(Y_test, num_authors)\n",
    "print 'test_size:', X_test.shape, Y_test.shape\n",
    "\n",
    "m.batch_size = 32*100\n",
    "m.set_mode(MiniBatcher.TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network\n",
    "\n",
    "#### Current architecture\n",
    "1. Convolution2D (48, 12, 12) + Relu + MaxPool (2,2)\n",
    "2. Convolution2D (48, 6, 6 ) + Relu + MaxPool (2,2)\n",
    "3. Convolution2D->1D (48, 6, 35) + Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(48, 12, 12,\n",
    "                    border_mode='full',\n",
    "                    input_shape=(1, 120, 120),\n",
    "                    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(48, 6, 6,\n",
    "                       border_mode='full',\n",
    "                       activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(MaxPooling2D(pool_size=(70,2)))\n",
    "\n",
    "model.add(Convolution2D(48, 6, 35, activation='relu'))\n",
    "model.add(Squeeze())\n",
    "model.add(Transpose3((0,2,1)))\n",
    "\n",
    "model.add(LSTM(output_dim=48, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "model.add(Dense(40, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.015, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adagrad')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "print \"Finished compilation with optimization set to SGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (X_train, Y_train) = m.get_batch()\n",
    "# X_train = np.expand_dims(X_train, 1)\n",
    "# # X_train = X_train[0:240]\n",
    "# Y_train = to_categorical(Y_train, num_authors)\n",
    "# activations = model.predict(X_train, verbose = 1)\n",
    "# print str(activations.shape)\n",
    "model.load_weights('basic_recurrent300.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.batch_size = 32*100\n",
    "m.set_mode(MiniBatcher.TRAIN)\n",
    "for i in range(500):\n",
    "    print 'Starting Epoch: ', i\n",
    "    start_time = time.time()\n",
    "    (X_train, Y_train) = m.get_batch()\n",
    "    X_train = np.expand_dims(X_train, 1)\n",
    "    Y_train = to_categorical(Y_train, num_authors)\n",
    "    print X_train.shape, Y_train.shape\n",
    "    model.fit(X_train, Y_train, batch_size=32, nb_epoch=1, show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n",
    "    print 'Elapsed Time: ', time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('basic_recurrent.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
