{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('/work/draw')\n",
    "FORMAT = '[%(asctime)s] %(name)-15s %(message)s'\n",
    "DATEFMT = \"%H:%M:%S\"\n",
    "logging.basicConfig(format=FORMAT, datefmt=DATEFMT, level=logging.INFO)\n",
    "\n",
    "#from theano.misc.pkl_utils import dump\n",
    "#from theano.misc.pkl_utials import load\n",
    "import os\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import fuel\n",
    "import ipdb\n",
    "import time\n",
    "import cPickle\n",
    "import h5py\n",
    "\n",
    "import random\n",
    "from skimage.filters import threshold_adaptive\n",
    "from skimage.draw import line\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from theano import tensor\n",
    "\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "from blocks.algorithms import GradientDescent, CompositeRule, StepClipping, RMSProp, Adam\n",
    "from blocks.bricks import Tanh, Identity, MLP, Linear, Rectifier, Softmax, Logistic\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.bricks.recurrent import SimpleRecurrent, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Orthogonal \n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, ProgressBar\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "from blocks.extensions.monitoring import DataStreamMonitoring, TrainingDataMonitoring\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "\n",
    "####TODO get extras working\n",
    "try:\n",
    "    from blocks.extras import Plot\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "import draw.datasets as datasets\n",
    "from draw.draw import *\n",
    "from draw.samplecheckpoint import SampleCheckpoint\n",
    "from draw.partsonlycheckpoint import PartsOnlyCheckpoint\n",
    "\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def RMSprop(params, cost, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iamMain = h5py.File('/fileserver/iam/iam-binary/iam_author_lines_bin.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frags = []\n",
    "auths = []\n",
    "i=0\n",
    "sizeCutHt=100\n",
    "sizeCutWt=200\n",
    "for auth in iamMain.keys():\n",
    "    for frag in iamMain[auth]:\n",
    "        fragShape = np.asarray(iamMain[auth][frag]).shape\n",
    "        if fragShape[0]>sizeCutHt and fragShape[1]>sizeCutWt:\n",
    "            image4frags = np.asarray(iamMain[auth][frag])\n",
    "            frags.append(image4frags)\n",
    "            auths.append(str(auth))\n",
    "        else:\n",
    "            pass\n",
    "    i+=1\n",
    "    if i%100==0:\n",
    "        print('auth %s is done' % auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_random_patch(images, patchSize, varThresh = 0.03):\n",
    "    patches = []\n",
    "    \n",
    "    if len(images)!=32:\n",
    "        ht, wt = images.shape\n",
    "        htIndx = ht - patchSize[0]\n",
    "        wtIndx = wt - patchSize[1]\n",
    "        randHt = random.sample(xrange(htIndx), 1)[0]\n",
    "        randWt = random.sample(xrange(wtIndx), 1)[0]\n",
    "        vari = 0\n",
    "        #imgVar = np.var(images/255.0)\n",
    "        \n",
    "        while vari<=varThresh:#*imgVar:\n",
    "            randHt = random.sample(xrange(htIndx), 1)[0]\n",
    "            randWt = random.sample(xrange(wtIndx), 1)[0]\n",
    "            imgPatch = images[randHt:randHt+patchSize[0], randWt:randWt+patchSize[1]]\n",
    "            vari = np.var(imgPatch/255.0)\n",
    "        #imgPatch = threshold_adaptive(imgPatch, 40, offset = 0.1 )\n",
    "        #print(vari)\n",
    "        patches = imgPatch.flatten()/255.0#.flatten())\n",
    "    \n",
    "    else:\n",
    "        for img in images:\n",
    "            ht, wt = img.shape\n",
    "            htIndx = ht - patchSize[0]\n",
    "            wtIndx = wt - patchSize[1]\n",
    "            randHt = random.sample(xrange(htIndx), 1)[0]\n",
    "            randWt = random.sample(xrange(wtIndx), 1)[0]\n",
    "            vari = 0\n",
    "            #imgVar = np.var(img/255.0)\n",
    "            \n",
    "            while vari<=varThresh:#*imgVar:\n",
    "                randHt = random.sample(xrange(htIndx), 1)[0]\n",
    "                randWt = random.sample(xrange(wtIndx), 1)[0]\n",
    "                imgPatch = img[randHt:randHt+patchSize[0], randWt:randWt+patchSize[1]]\n",
    "                vari = np.var(imgPatch/255.0)\n",
    "            \n",
    "            #imgPatch = threshold_adaptive(imgPatch, 40, offset = 0.1 )\n",
    "            #print(vari)\n",
    "            patches.append(imgPatch.flatten()/255.0)#.flatten())\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pickleModel(trainedClassifier, file2save2 = None, filePath='/work/notebooks/drawModels/', fileName = 'myModels'):\n",
    "    \n",
    "    if file2save2 == None:\n",
    "        finFile = filePath+fileName+'.zip'\n",
    "    else:\n",
    "        finFile = filePath+file2save2\n",
    "    \n",
    "    with open(finFile, 'wb') as f:\n",
    "        cPickle.dump(trainedClassifier, f)\n",
    "\n",
    "        \n",
    "def loadModel(filePath):\n",
    "    with open(filePath, 'rb') as f:\n",
    "        model = cPickle.load(f)\n",
    "    \n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load = False\n",
    "name = 'draw'\n",
    "dataset = 'iam'\n",
    "epochs = 200000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "attention = '15,15' #number of read, write filters\n",
    "n_iter = 64 #glimpses\n",
    "enc_dim = 800 #timesteps for encoder RNN\n",
    "dec_dim = 800 #timesteps for deocder RNN\n",
    "z_dim = 200 #\n",
    "image_size = (100,200)\n",
    "channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_height, img_width = image_size\n",
    "x_dim = channels * img_height * img_width\n",
    "\n",
    "mlpinits = {\n",
    "    #'weights_init': Orthogonal(),\n",
    "    'weights_init': IsotropicGaussian(0.01),\n",
    "    'biases_init': Constant(0.),\n",
    "}\n",
    "    \n",
    "rnninits = {\n",
    "    #'weights_init': Orthogonal(),\n",
    "    'weights_init': IsotropicGaussian(0.01),\n",
    "    'biases_init': Constant(0.),\n",
    "}\n",
    "inits = {\n",
    "    #'weights_init': Orthogonal(),\n",
    "    'weights_init': IsotropicGaussian(0.01),\n",
    "    'biases_init': Constant(0.),\n",
    "}\n",
    "\n",
    "# Configure attention mechanism\n",
    "if attention != \"\":\n",
    "    read_N, write_N = attention.split(',')\n",
    "\n",
    "    read_N = int(read_N)\n",
    "    write_N = int(write_N)\n",
    "    read_dim = 2 * channels * read_N ** 2\n",
    "\n",
    "    reader = AttentionReader(x_dim=x_dim, dec_dim=dec_dim,\n",
    "                             channels=channels, width=img_width, height=img_height,\n",
    "                             N=read_N, **inits)\n",
    "    writer = AttentionWriter(input_dim=dec_dim, output_dim=x_dim,\n",
    "                             channels=channels, width=img_width, height=img_height,\n",
    "                             N=write_N, **inits)\n",
    "    attention_tag = \"r%d-w%d\" % (read_N, write_N)\n",
    "else:\n",
    "    read_dim = 2*x_dim\n",
    "\n",
    "    reader = Reader(x_dim=x_dim, dec_dim=dec_dim, **inits)\n",
    "    writer = Writer(input_dim=dec_dim, output_dim=x_dim, **inits)\n",
    "\n",
    "    attention_tag = \"full\"\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "if name is None:\n",
    "    name = dataset\n",
    "\n",
    "# Learning rate\n",
    "def lr_tag(value):\n",
    "    \"\"\" Convert a float into a short tag-usable string representation. E.g.:\n",
    "        0.1   -> 11\n",
    "        0.01  -> 12\n",
    "        0.001 -> 13\n",
    "        0.005 -> 53\n",
    "    \"\"\"\n",
    "    exp = np.floor(np.log10(value))\n",
    "    leading = (\"%e\"%value)[0]\n",
    "    return \"%s%d\" % (leading, -exp)\n",
    "\n",
    "lr_str = lr_tag(learning_rate)\n",
    "\n",
    "subdir = name + \"-\" + time.strftime(\"%Y%m%d-%H%M%S\");\n",
    "longname = \"%s-%s-t%d-enc%d-dec%d-z%d-lr%s\" % (dataset, attention_tag, n_iter, enc_dim, dec_dim, z_dim, lr_str)\n",
    "pickle_file = subdir + \"/\" + longname + \".pkl\"\n",
    "\n",
    "print(\"\\nRunning experiment %s\" % longname)\n",
    "print(\"               dataset: %s\" % dataset)\n",
    "print(\"          subdirectory: %s\" % subdir)\n",
    "print(\"         learning rate: %g\" % learning_rate)\n",
    "print(\"             attention: %s\" % attention)\n",
    "print(\"          n_iterations: %d\" % n_iter)\n",
    "print(\"     encoder dimension: %d\" % enc_dim)\n",
    "print(\"           z dimension: %d\" % z_dim)\n",
    "print(\"     decoder dimension: %d\" % dec_dim)\n",
    "print(\"            batch size: %d\" % batch_size)\n",
    "print(\"                epochs: %d\" % epochs)\n",
    "print()\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "encoder_rnn = LSTM(dim=enc_dim, name=\"RNN_enc\", **rnninits)\n",
    "decoder_rnn = LSTM(dim=dec_dim, name=\"RNN_dec\", **rnninits)\n",
    "encoder_mlp = MLP([Identity()], [(read_dim+dec_dim), 4*enc_dim], name=\"MLP_enc\", **inits)\n",
    "decoder_mlp = MLP([Identity()], [             z_dim, 4*dec_dim], name=\"MLP_dec\", **inits)\n",
    "q_sampler = Qsampler(input_dim=enc_dim, output_dim=z_dim, **inits)\n",
    "\n",
    "draw = DrawModel(\n",
    "            n_iter, \n",
    "            reader=reader,\n",
    "            encoder_mlp=encoder_mlp,\n",
    "            encoder_rnn=encoder_rnn,\n",
    "            sampler=q_sampler,\n",
    "            decoder_mlp=decoder_mlp,\n",
    "            decoder_rnn=decoder_rnn,\n",
    "            writer=writer)\n",
    "draw.initialize()\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "x = tensor.matrix('features')\n",
    "n_samplings = tensor.scalar('n_samplings', dtype=theano.config.floatX)\n",
    "#y = tensor.matrix('targets')\n",
    "\n",
    "#x_recons = reconstructed image, i.e. the final canvas, with shape \n",
    "#kl_terms = CHECK\n",
    "#canvas = is the collection of cumulative canvases, i.e. c+=c + self.writer.apply(h_dec)\n",
    "            # shape (n_iter, batch_size, flattenXdim) \n",
    "            # the last is used to calculate x_recons \n",
    "#h_enc = encoded vector from encoder RNN, with shape (n_iter, batch_size, enc_dim)\n",
    "#c_enc = (n_iter, batch_size, enc_dim)\n",
    "#z = (n_iter, batch_size, z_dim)\n",
    "    \n",
    "x_recons, kl_terms, canvas, h_enc, c_enc, z, h_dec, c_dec = draw.reconstructMORE(x)\n",
    "\n",
    "#mlp = MLP(activations = [Logistic(), Softmax()], dims = [enc_dim,1024,len(authDict)], \n",
    "#         **mlpinits)\n",
    "#mlp.name = 'mlp'\n",
    "#mlp.initialize()\n",
    "#pYx = mlp.apply(z[-1])\n",
    "\n",
    "#soft_term = CategoricalCrossEntropy().apply(y, pYx)\n",
    "#soft_term.name = 'soft_term'\n",
    "\n",
    "recons_term = BinaryCrossEntropy().apply(x, x_recons) #+ CategoricalCrossEntropy().apply(y, pYx)\n",
    "recons_term.name = \"recons_term\"\n",
    "\n",
    "cost = recons_term + kl_terms.sum(axis=0).mean() + soft_term \n",
    "cost.name = \"nll_bound\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cg = ComputationGraph([cost])\n",
    "#cg.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = VariableFilter(roles=[PARAMETER])(cg.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#updates = RMSprop(params, cost, learning_rate, c=10)\n",
    "updates = Adam(params, cost, learning_rate, c=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if load:\n",
    "    train = loadModel('/work/notebooks/drawModels/draw_drawlikeIAMTRAIN190000.zip')\n",
    "    predict = loadModel('/work/notebooks/drawModels/draw_drawlikeIAMPREDICT190000.zip')\n",
    "    drawRandom = loadModel('/work/notebooks/drawModels/draw_drawlikeIAMDRAWRANDOM190000.zip')\n",
    "else:\n",
    "    train = theano.function([x], [cost, canvas, h_enc, c_enc, z, h_dec, c_dec, x_recons], \n",
    "                            updates = updates, allow_input_downcast=True)\n",
    "    predict = theano.function([x], [canvas, h_enc, c_enc, z, h_dec, c_dec, x_recons], allow_input_downcast=True)\n",
    "    drawRandom = theano.function([], draw.sample(1), allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holdoutFrag = []\n",
    "holdoutAuth = []\n",
    "for hold in [4, 7032, 11133]:\n",
    "    holdoutFrag.append(frags[hold])\n",
    "    holdoutAuth.append(frags[hold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del frags[4]\n",
    "del frags[7031]\n",
    "del frags[11131]\n",
    "del auths[4]\n",
    "del auths[7031]\n",
    "del auths[11131]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randIndex = random.sample(xrange(len(frags)), len(frags))\n",
    "frags = [frags[x] for x in randIndex]\n",
    "auths = [auths[x] for x in randIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneHotOutput(miniAuths, authDict):\n",
    "    numAuths = len(authDict)\n",
    "    numInMini = len(miniAuths)\n",
    "    \n",
    "    miniOut = np.zeros((numInMini, numAuths), dtype=theano.config.floatX)\n",
    "    \n",
    "    for auth in xrange(numInMini):\n",
    "        miniOut[auth, authDict[miniAuths[auth]]] = 1\n",
    "        \n",
    "    return miniOut\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#min of one image shape is 82\n",
    "runname = 'binaryBeginnings'\n",
    "epochCost = []\n",
    "\n",
    "epochs = 200000\n",
    "batch_size = 32\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    print(' ')\n",
    "    costCollect = []\n",
    "    print (\"EPOCH: \", epoch)\n",
    "    #print ('   iteration: ', iteration)\n",
    "    #if epoch%5 == 0:\n",
    "        #random.shuffle(frags)\n",
    "        #print ('data shuffled')\n",
    "    for start, end in zip(range(0, len(frags),batch_size), range(batch_size, len(frags), batch_size)):\n",
    "        #inputs,_ = exfromem(trainSet, start, end)\n",
    "        inputs = frags[start:end]\n",
    "        patches = get_random_patch(inputs, image_size)\n",
    "        #outputs = oneHotOutput(auths[start:end], authDict)\n",
    "        trainOut = train(patches)#, outputs)\n",
    "        costCollect.append(trainOut[0])\n",
    "        \n",
    "        #recon = reconstructed(patches)\n",
    "    \n",
    "        if iteration%100==0:\n",
    "            print ('   iteration: ', iteration)\n",
    "            print (\"   cost: \", trainOut[0])\n",
    "        #trainOut[cost, canvas, h_enc, c_enc, z, h_dec, c_dec, x_recons]\n",
    "        \n",
    "        if iteration%700 == 0:\n",
    "            patchCollect = []\n",
    "            #predict = [canvas, h_enc, c_enc, z, h_dec, c_dec, x_recons]\n",
    "            for i in xrange(10):\n",
    "                zPatch = get_random_patch(inputs[0], image_size)\n",
    "                #predictPatch = predict(zPatch)[3][-1]\n",
    "                patchCollect.append(predict(zPatch.reshape(1,image_size[0]*image_size[1]))[3][-1])\n",
    "            \n",
    "            patchCollect = np.mean(np.asarray(patchCollect), axis = 0)\n",
    "            \n",
    "            plt.figure(1, figsize = (20,20))\n",
    "            plt.subplot(1,4,1)\n",
    "            plt.imshow(patches[0].reshape(*image_size), cmap = 'gray')\n",
    "\n",
    "            plt.subplot(1,4,2)\n",
    "            plt.imshow(trainOut[-1][0].reshape(*image_size), cmap = 'gray')\n",
    "            \n",
    "            #plt.subplot(1,5,3)\n",
    "            #plt.imshow(drawLikeX(patches[0].reshape(1,14000))[-1].reshape(*image_size), cmap = 'gray')\n",
    "            \n",
    "            plt.subplot(1,4,3)\n",
    "            plt.imshow(drawRandom()[-1].reshape(*image_size), cmap = 'gray')\n",
    "            \n",
    "            plt.subplot(1,4,4)\n",
    "            plt.imshow(decodering(patchCollect.reshape(1,1,z_dim))[0].reshape(*image_size), cmap = 'gray')\n",
    "            plt.show()\n",
    "            \n",
    "            ####PICKLE MODEL\n",
    "        if iteration%1000 == 0:\n",
    "            #predOut = predict(patches)\n",
    "            #print (\"train accuracy: \", np.mean(np.argmax(predOut[0], axis = 1) == np.argmax(outputs, axis = 1)))\n",
    "            #print ('targets: ', np.argmax(outputs, axis = 1))\n",
    "            #print ('predictions: ', np.argmax(predOut[0], axis = 1))\n",
    "            #print ('raw pred: ', predOut[0])\n",
    "            plt.plot(epochCost[10:])\n",
    "            plt.show()\n",
    "            \n",
    "            pickleModel(train, filePath='/work/notebooks/drawModels/', \n",
    "                        fileName = runname+'TRAIN'+str(iteration))\n",
    "            pickleModel(predict, filePath='/work/notebooks/drawModels/', \n",
    "                        fileName = runname+'PREDICT'+str(iteration))\n",
    "            pickleModel(drawLikeX, filePath='/work/notebooks/drawModels/', \n",
    "                        fileName = runname+'DRAWLIKEX'+str(iteration))\n",
    "            pickleModel(drawRandom, filePath='/work/notebooks/drawModels/', \n",
    "                        fileName = runname+'DRAWRANDOM'+str(iteration))\n",
    "            pickleModel(decodering, filePath='/work/notebooks/drawModels/', \n",
    "                        fileName = runname+'DECODERING'+str(iteration))\n",
    "            \n",
    "            ####SAVE PNG\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE        \n",
    "    epochCost.append(np.mean(costCollect))\n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")\n",
    "    print ('Epoch cost average:', np.mean(costCollect)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reading_square(muX, muY, imgshp = [28,28], fill=True):\n",
    "        muX = np.clip(muX, 0, imgshp[1]-1)\n",
    "        muY = np.clip(muY, 0, imgshp[0]-1)\n",
    "        l = muX.shape[0]\n",
    "        c = np.zeros([l,imgshp[0],imgshp[1]])\n",
    "        muX = np.floor(muX)\n",
    "        muY = np.floor(muY)\n",
    "        for i in range(l):\n",
    "            for y in muY[i]:\n",
    "                for x in muX[i]:\n",
    "                    c[i,y,x] = 1\n",
    "\n",
    "\n",
    "        if fill:\n",
    "            maxX, minX = np.max(muX,axis=1), np.min(muX, axis=1)\n",
    "            maxY, minY = np.max(muY,axis=1), np.min(muY, axis=1)\n",
    "            for i in range(l):\n",
    "                c[i, minY[i]:maxY[i], minX[i]:maxX[i]] = 1\n",
    "\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_n_by_n_images(images,epoch=None,folder=None, n = 10, shp=[28,28]):\n",
    "    \"\"\" Plot 100 MNIST images in a 10 by 10 table. Note that we crop\n",
    "    the images so that they appear reasonably close together. The\n",
    "    image is post-processed to give the appearance of being continued.\"\"\"\n",
    "    #image = np.concatenate(images, axis=1)\n",
    "    i = 0\n",
    "    a,b = shp\n",
    "    img_out = np.zeros((a*n, b*n))\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            xa,xb = x*a, (x+1)*b\n",
    "            ya,yb = y*a, (y+1)*b\n",
    "            im = np.reshape(images[i], (a,b))\n",
    "            img_out[xa:xb, ya:yb] = im\n",
    "            i+=1\n",
    "    #matshow(img_out*100.0, cmap = matplotlib.cm.binary)\n",
    "    img_out = (255*img_out).astype(np.uint8)\n",
    "    img_out = Image.fromarray(img_out)\n",
    "    if folder is not None and epoch is not None:\n",
    "        img_out.save(os.path.join(folder,epoch + \".png\"))\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchIterator(object):\n",
    "    \"\"\"\n",
    "     Cyclic Iterators over batch indexes. Permutes and restarts at end\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_indices, batchsize, data, testing=False, process_func=None):\n",
    "        if isinstance(batch_indices, int):\n",
    "            self.n = batch_indices\n",
    "            self.batchidx = np.arange(batch_indices)\n",
    "        else:\n",
    "            self.n = len(batch_indices)\n",
    "            self.batchidx = np.array(batch_indices)\n",
    "\n",
    "        self.batchsize = batchsize\n",
    "        self.testing = testing\n",
    "        if process_func is None:\n",
    "            process_func = lambda x:x\n",
    "        self.process_func = process_func\n",
    "\n",
    "        if not isinstance(data, (list, tuple)):\n",
    "            data = [data]\n",
    "\n",
    "        self.data = data\n",
    "        if not self.testing:\n",
    "            self.createindices = lambda: np.random.permutation(self.n)\n",
    "        else: # testing == true\n",
    "            assert self.n % self.batchsize == 0, \"for testing n must be multiple of batch size\"\n",
    "            self.createindices = lambda: range(self.n)\n",
    "\n",
    "        self.perm = self.createindices()\n",
    "        assert self.n > self.batchsize\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def _get_permuted_batches(self,n_batches):\n",
    "        # return a list of permuted batch indeces\n",
    "        batches = []\n",
    "        for i in range(n_batches):\n",
    "\n",
    "            # extend random permuation if shorter than batchsize\n",
    "            if len(self.perm) <= self.batchsize:\n",
    "                new_perm = self.createindices()\n",
    "                self.perm = np.hstack([self.perm, new_perm])\n",
    "\n",
    "            batches.append(self.perm[:self.batchsize])\n",
    "            self.perm = self.perm[self.batchsize:]\n",
    "        return batches\n",
    "\n",
    "    def next(self):\n",
    "        batch = self._get_permuted_batches(1)[0]   # extract a single batch\n",
    "        data_batches = [self.process_func(data_n[batch]) for data_n in self.data]\n",
    "        return data_batches\n",
    "    \n",
    "    \n",
    "def threaded_generator(generator, num_cached=50):\n",
    "    # this code is writte by jan Schluter\n",
    "    # copied from https://github.com/benanne/Lasagne/issues/12\n",
    "    import Queue\n",
    "    queue = Queue.Queue(maxsize=num_cached)\n",
    "    sentinel = object()  # guaranteed unique reference\n",
    "\n",
    "    # define producer (putting items into queue)\n",
    "    def producer():\n",
    "        for item in generator:\n",
    "            queue.put(item)\n",
    "        queue.put(sentinel)\n",
    "\n",
    "    # start producer (in a background thread)\n",
    "    import threading\n",
    "    thread = threading.Thread(target=producer)\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "\n",
    "    # run as consumer (read items from queue, in current thread)\n",
    "    item = queue.get()\n",
    "    while item is not sentinel:\n",
    "        yield item\n",
    "        queue.task_done()\n",
    "        item = queue.get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
