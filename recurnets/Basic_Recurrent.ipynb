{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Recurrent Neural Network\n",
    "\n",
    "Testing out original code for a simple LSTM to understand the sequential writing of an author from left to right. (To do: bi-directional recurrent LSTMs.)\n",
    "\n",
    "Details: \n",
    "We require two additional layers that I've written to make the dimensions of the input to other layers consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980M (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "import keras\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "from keras.layers.core import Layer\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "import theano.tensor as T\n",
    "\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append('../repo/d-script/')\n",
    "# d-script imports\n",
    "from data_iters.minibatcher import MiniBatcher\n",
    "from data_iters.iam_hdf5_iterator import IAM_MiniBatcher\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Keras layers for use in the recurrent network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Squeeze(Layer):\n",
    "    '''\n",
    "        Get rid of any dimensions of size 1.\n",
    "        First dimension is assumed to be nb_samples.\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Squeeze, self).__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        input_shape = self.input_shape\n",
    "        data_shape = tuple( np.array(input_shape)[ np.array(input_shape) > 1 ] )\n",
    "        return (input_shape[0],)+ data_shape\n",
    "\n",
    "    def get_output(self, train=False):\n",
    "        X = self.get_input(train)\n",
    "        # size = T.prod(X.shape) // X.shape[0]\n",
    "        # nshape = (X.shape[0], size)\n",
    "        # return T.reshape(X, output_shape)\n",
    "        return X.squeeze()\n",
    "    \n",
    "class Transpose3(Layer):\n",
    "    '''\n",
    "        Get rid of any dimensions of size 1.\n",
    "        First dimension is assumed to be nb_samples.\n",
    "    '''\n",
    "    def __init__(self, transpose_order, **kwargs):\n",
    "        self.transpose_order = transpose_order\n",
    "        super(Transpose3, self).__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        input_shape = self.input_shape\n",
    "        data_shape = ()\n",
    "        for j in self.transpose_order:\n",
    "            data_shape+=(input_shape[j],)\n",
    "        return data_shape\n",
    "\n",
    "    def get_output(self, train=False):\n",
    "        X = self.get_input(train)\n",
    "        # size = T.prod(X.shape) // X.shape[0]\n",
    "        # nshape = (X.shape[0], size)\n",
    "        # return T.reshape(X, output_shape)\n",
    "        return X.transpose(self.transpose_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data (40 authors, 15 forms per author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_authors=40\n",
    "num_forms_per_author=15\n",
    "hdf5_file = '/memory/author_lines.hdf5'\n",
    "shingle_dim=(120,120)\n",
    "batch_size=32\n",
    "use_form=True\n",
    "\n",
    "iam_m = IAM_MiniBatcher(hdf5_file, num_authors, num_forms_per_author, shingle_dim=shingle_dim, use_form=use_form, default_mode=MiniBatcher.TRAIN, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network\n",
    "\n",
    "#### Current architecture\n",
    "1. Convolution2D (48, 12, 12) + Relu + MaxPool (2,2)\n",
    "2. Convolution2D (48, 6, 6 ) + Relu + MaxPool (2,2)\n",
    "3. Convolution2D->1D (48, 6, 35) + Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(48, 12, 12,\n",
    "                    border_mode='full',\n",
    "                    input_shape=(1, 120, 120),\n",
    "                    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(48, 6, 6,\n",
    "                       border_mode='full',\n",
    "                       activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(MaxPooling2D(pool_size=(70,2)))\n",
    "\n",
    "model.add(Convolution2D(48, 6, 35, activation='relu'))\n",
    "model.add(Squeeze())\n",
    "model.add(Transpose3((0,2,1)))\n",
    "\n",
    "model.add(LSTM(output_dim=48, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "model.add(Dense(40, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished compilation with optimization set to SGD\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.015, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adagrad')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "print \"Finished compilation with optimization set to SGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the first validation batch\n",
      "Finished getting 320 data points\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights('basic_recurrent300.hd5')\n",
    "\n",
    "print \"Getting the first validation batch\"\n",
    "[X_val, Y_val] = iam_m.get_val_batch(batch_size*100)\n",
    "X_val = np.expand_dims(X_val, 1)\n",
    "Y_val = to_categorical(Y_val, num_authors)\n",
    "print \"Finished getting \"+str(batch_size*10)+\" data points\"\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "def randangle(batch):\n",
    "    newbatch = np.zeros(batch.shape)\n",
    "    for i,im in enumerate(batch):\n",
    "        imangle = np.asarray(Image.fromarray(im.squeeze()).rotate(7.5*np.random.randn()))\n",
    "        newbatch[i]=imangle\n",
    "    return newbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch iteration 0/500 on 40 authors.\n",
      "Train on 32000 samples, validate on 3200 samples\n",
      "Epoch 1/1\n",
      " 4672/32000 [===>..........................] - ETA: 554s - loss: 3.6802 - acc: 0.0330"
     ]
    }
   ],
   "source": [
    "total_iters = 500\n",
    "for batch_iter in xrange(total_iters):\n",
    "    print \"Data load \"+str(batch_size*1000)+\" authors\"\n",
    "    [X_train, Y_train] = iam_m.get_train_batch(batch_size*1000)\n",
    "    X_train = np.expand_dims(X_train, 1)\n",
    "    X_train = randangle(X_train)\n",
    "    Y_train = to_categorical(Y_train, num_authors)\n",
    "    print \"Batch iteration \"+str(batch_iter)+\"/\"+str(total_iters)+\" on \"+str(num_authors)+\" authors.\"\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1, show_accuracy=True, verbose=1, validation_data=(X_val, Y_val))\n",
    "    if (batch_iter % 100)==0 and batch_iter != 0:\n",
    "        model.save_weights('recnet.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('basic_recurrent.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
