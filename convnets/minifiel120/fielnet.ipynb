{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from optparse import OptionParser\n",
    "\n",
    "# Required libraries\n",
    "import h5py\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../d-script/')\n",
    "# d-script imports\n",
    "from data_iters.minibatcher import MiniBatcher\n",
    "from data_iters.archive.iam_iterator import IAM_MiniBatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level document sharding\n",
    "#### Be sure to set use_form=True due to HDF5 structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "unable to open file (File accessibilty: Unable to open file)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b8c501b23117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Read from memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mhdf5_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/fileserver/iam/iam-processed/lines/lines.hdf5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mauthor_hdf5_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/memory/author_lines.hdf5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfile_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, **kwds)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mGroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDONLY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (h5py/h5f.c:1806)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: unable to open file (File accessibilty: Unable to open file)"
     ]
    }
   ],
   "source": [
    "# Read from disk\n",
    "# hdf5_file = h5py.File('/fileserver/iam/iam-processed/lines/lines.hdf5','r')\n",
    "# author_hdf5_file = h5py.File('/fileserver/iam/iam-processed/lines/author_lines.hdf5','r')\n",
    "\n",
    "# Read from memory\n",
    "hdf5_file = h5py.File('/fileserver/iam/iam-processed/lines/lines.hdf5','r')\n",
    "author_hdf5_file = h5py.File('/memory/author_lines.hdf5','r')\n",
    "\n",
    "file_list = hdf5_file.keys()\n",
    "num_authors=64\n",
    "# num_forms_per_author=-1\n",
    "num_forms_per_author=500\n",
    "shingle_dim=(120,1909)\n",
    "shingle_dim=(120,120)\n",
    "use_form=True\n",
    "\n",
    "batch_size=32\n",
    "lr = 0.01\n",
    "total_iters=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the minibatcher and check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a shingle from a line with dimension shingle_dim\n",
    "def get_shingle(original_line, shingle_dim):\n",
    "    # Pull shingle from the line\n",
    "    (height, width) = original_line.shape\n",
    "    max_x = max(width - shingle_dim[1], 0)\n",
    "    max_y = max(height - shingle_dim[0], 0)\n",
    "    x_start = random.randint(0, max_x)\n",
    "    y_start = random.randint(0, max_y)\n",
    "    if width < shingle_dim[1] or height < shingle_dim[0]: # The line is too small in at least one access\n",
    "        output_arr = np.zeros(shingle_dim)\n",
    "        output_arr.fill(255)\n",
    "        output_arr[:height,:width] = original_line[:min(height, shingle_dim[0]), :min(width, shingle_dim[1])]\n",
    "        return output_arr\n",
    "    else:\n",
    "        return original_line[y_start:y_start+ shingle_dim[0], x_start:x_start+shingle_dim[1]]\n",
    "\n",
    "# Return a list of authors with at least min_records number of lines\n",
    "def author_list(author_hdf5_file, min_records=-1):\n",
    "    author_list = author_hdf5_file.keys()\n",
    "    author_num = 0\n",
    "    author_ids = {}\n",
    "    for authors in author_list:\n",
    "        num_records = len( author_hdf5_file[authors].keys() )\n",
    "        if num_records > min_records:\n",
    "            author_ids[authors] = author_num\n",
    "            author_num += 1\n",
    "    print \"There are \"+str(author_num)+\" authors with \"+str(min_records)+\" records.\"\n",
    "    return author_ids\n",
    "\n",
    "# From a dictionary, get a random sample\n",
    "def sample_dictionary( the_dict ):\n",
    "    keys = the_dict.keys()\n",
    "    sampleno = np.random.randint(0, len(keys))\n",
    "    randkey = keys[sampleno]\n",
    "    return the_dict[ randkey ]\n",
    "\n",
    "# From an HDF5 file, a list of author ID's, return a minibatch\n",
    "def get_batch( author_hdf5_file, author_ids, shingle_size=(120,120), data_size=32 ):\n",
    "    \n",
    "    author_keys = author_ids.keys()\n",
    "    author_rands = np.random.randint(0, len(author_keys), data_size)\n",
    "\n",
    "    author_batch = np.zeros( (data_size, shingle_size[0], shingle_size[1]))\n",
    "    author_truth = np.zeros( data_size )\n",
    "    for i, author_rand in enumerate(author_rands):\n",
    "        author_group = author_hdf5_file[ author_keys[author_rand] ]\n",
    "        author_batch[i,:,:] = get_shingle( sample_dictionary( author_group ).value , shingle_size)\n",
    "        author_truth[i] = author_ids[ author_keys[author_rand] ]\n",
    "        \n",
    "    return author_batch, author_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup batch system to get authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_ids = author_list(author_hdf5_file, min_records=-1)\n",
    "author_batch, author_truth = get_batch( author_hdf5_file, author_ids )\n",
    "num_authors = len(author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# record = file_list[np.random.randint(0,len(file_list))]\n",
    "# im = hdf5_file[ record ].value\n",
    "shards, authors = get_batch(author_hdf5_file, author_ids)\n",
    "shard = shards[0]\n",
    "\n",
    "plt.figure(num=None, figsize=(8, 6), dpi=180, facecolor='w', edgecolor='k')\n",
    "plt.imshow(shard)\n",
    "# plt.figure(num=None, figsize=(16, 12), dpi=180, facecolor='w', edgecolor='k')\n",
    "# plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(96, 12, 12,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=(1, shingle_dim[0], shingle_dim[1])))\n",
    "\n",
    "    model.add(BN())\n",
    "    #model.add(PReLU())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(96, 6, 6))\n",
    "    model.add(BN())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(PReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution2D(256, 6, 6, border_mode = 'valid'))\n",
    "    model.add(BN())\n",
    "    model.add(Activation('relu'))\n",
    "    #    model.add(PReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #    #model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(256, 3, 3, border_mode = 'valid'))\n",
    "    model.add(BN())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(PReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(BN())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(BN())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_authors))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    print \"Compiling model\"\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.7, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "    print \"Finished compilation\"\n",
    "\n",
    "\n",
    "if False:\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(48, 12, 12,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=(1, shingle_dim[0], shingle_dim[1]),\n",
    "                        activation='relu'))\n",
    "\n",
    "    model.add(Convolution2D(48, 6, 6, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution2D(128, 6, 6, border_mode = 'valid', activation='relu'))\n",
    "    #model.add(BN(epsilon=1e-6))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(128, 4, 4, border_mode = 'valid', activation='relu'))\n",
    "    #model.add(BN(epsilon=1e-6))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode = 'valid', activation='relu'))\n",
    "    #model.add(BN(epsilon=1e-6))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_authors))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    print \"Compiling model\"\n",
    "    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "    print \"Finished compilation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def randangle(batch):\n",
    "    newbatch = np.zeros(batch.shape)\n",
    "    for i,im in enumerate(batch):\n",
    "        imangle = np.asarray(Image.fromarray(im.squeeze()).rotate(7.5*np.random.randn()))\n",
    "        newbatch[i]=imangle\n",
    "    return newbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for batch_iter in xrange(total_iters):\n",
    "    # [X_train, Y_train] = iam_m.get_train_batch(batch_size*100)\n",
    "    print \"Getting batch number \"+str(batch_iter)+\".\"\n",
    "    (X_train, Y_train) = get_batch(author_hdf5_file, author_ids, data_size=32*100)\n",
    "    X_train = 1.0 - X_train / 255.0\n",
    "    X_train = np.expand_dims(X_train, 1)\n",
    "    X_train = randangle(X_train)\n",
    "    Y_train = to_categorical(Y_train, num_authors)\n",
    "    print \"Batch iteration \"+str(batch_iter)+\"/\"+str(total_iters)+\" on \"+str(num_authors)+\" authors.\"\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1, show_accuracy=True, verbose=1) #, validation_data=(X_test, Y_test))\n",
    "    # model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1, show_accuracy=True, verbose=1, validation_data=(X_train, Y_train))\n",
    "    if (batch_iter % 20)==0 and batch_iter != 0:\n",
    "        model.save_weights('/work/code/models/fielnet-train.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    (X_train, Y_train) = get_batch(author_hdf5_file, author_ids)\n",
    "    X_train = 1.0 - X_train / 255.0\n",
    "    X_train = np.expand_dims(X_train, 1)\n",
    "    X_train = randangle(X_train)\n",
    "\n",
    "plt.subplots(4,4)\n",
    "for i in xrange(16):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.imshow(X_train[i][0])\n",
    "print \"number of authors = \"+str(num_authors) + \", batch size = \"+str(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
