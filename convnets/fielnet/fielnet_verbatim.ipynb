{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from optparse import OptionParser\n",
    "\n",
    "# Required libraries\n",
    "import h5py\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../d-script/')\n",
    "# d-script imports\n",
    "from data_iters.minibatcher import MiniBatcher\n",
    "from data_iters.iam_hdf5_iterator import IAM_MiniBatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level document sharding\n",
    "#### Be sure to set use_form=True due to HDF5 structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read from disk\n",
    "hdf5_file = h5py.File('/fileserver/iam/iam-processed/words/words.hdf5','r')\n",
    "author_hdf5_file = h5py.File('/fileserver/iam/iam-processed/words/author_words.hdf5','r')\n",
    "\n",
    "file_list = hdf5_file.keys()\n",
    "num_authors=64\n",
    "num_forms_per_author=-1\n",
    "shingle_dim=(56,56)\n",
    "use_form=True\n",
    "\n",
    "batch_size=32\n",
    "lr = 0.01\n",
    "total_iters=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the minibatcher and check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a shingle from a line with dimension shingle_dim\n",
    "def get_shingle(original_line, shingle_dim):\n",
    "    # Pull shingle from the line\n",
    "    (height, width) = original_line.shape\n",
    "    max_x = max(width - shingle_dim[1], 0)\n",
    "    max_y = max(height - shingle_dim[0], 0)\n",
    "    x_start = random.randint(0, max_x)\n",
    "    y_start = random.randint(0, max_y)\n",
    "    if width < shingle_dim[1] or height < shingle_dim[0]: # The line is too small in at least one access\n",
    "        output_arr = np.zeros(shingle_dim)\n",
    "        output_arr.fill(255)\n",
    "        output_arr[:height,:width] = original_line[:min(height, shingle_dim[0]), :min(width, shingle_dim[1])]\n",
    "        return output_arr\n",
    "    else:\n",
    "        return original_line[y_start:y_start+ shingle_dim[0], x_start:x_start+shingle_dim[1]]\n",
    "\n",
    "# Return a list of authors with at least min_records number of lines\n",
    "def author_list(author_hdf5_file, min_records=-1):\n",
    "    author_list = author_hdf5_file.keys()\n",
    "    author_num = 0\n",
    "    author_ids = {}\n",
    "    for authors in author_list:\n",
    "        num_records = len( author_hdf5_file[authors].keys() )\n",
    "        if num_records > min_records:\n",
    "            author_ids[authors] = author_num\n",
    "            author_num += 1\n",
    "    print \"There are \"+str(author_num)+\" authors with \"+str(min_records)+\" records.\"\n",
    "    return author_ids\n",
    "\n",
    "# From a dictionary, get a random sample\n",
    "def sample_dictionary( the_dict ):\n",
    "    keys = the_dict.keys()\n",
    "    sampleno = np.random.randint(0, len(keys))\n",
    "    randkey = keys[sampleno]\n",
    "    return the_dict[ randkey ]\n",
    "\n",
    "# From an HDF5 file, a list of author ID's, return a minibatch\n",
    "def get_batch( author_hdf5_file, author_ids, shingle_size=(120,120), data_size=32 ):\n",
    "    \n",
    "    author_keys = author_ids.keys()\n",
    "    author_rands = np.random.randint(0, len(author_keys), data_size)\n",
    "\n",
    "    author_batch = np.zeros( (data_size, shingle_size[0], shingle_size[1]))\n",
    "    author_truth = np.zeros( data_size )\n",
    "    for i, author_rand in enumerate(author_rands):\n",
    "        author_group = author_hdf5_file[ author_keys[author_rand] ]\n",
    "        author_batch[i,:,:] = get_shingle( sample_dictionary( author_group ).value , shingle_size)\n",
    "        author_truth[i] = author_ids[ author_keys[author_rand] ]\n",
    "        \n",
    "    return author_batch, author_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup batch system to get authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_ids = author_list(author_hdf5_file, min_records=num_forms_per_author)\n",
    "author_batch, author_truth = get_batch( author_hdf5_file, author_ids )\n",
    "num_authors = len(author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shards, authors = get_batch(author_hdf5_file, author_ids)\n",
    "shard = shards[0]\n",
    "\n",
    "plt.figure(num=None, figsize=(8, 6), dpi=180, facecolor='w', edgecolor='k')\n",
    "plt.imshow(shard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(96, 12, 12,\n",
    "                    border_mode='valid',\n",
    "                    input_shape=(1, shingle_dim[0], shingle_dim[1]),\n",
    "                    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(256, 6, 6, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(256, 3, 3, border_mode = 'valid', activation='relu'))\n",
    "\n",
    "model.add(Convolution2D(256, 3, 3, border_mode = 'valid', activation='relu'))\n",
    "\n",
    "model.add(Convolution2D(256, 3, 3, border_mode = 'valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(num_authors))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print \"Compiling model\"\n",
    "sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "print \"Finished compilation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def randangle(batch):\n",
    "    newbatch = np.zeros(batch.shape)\n",
    "    for i,im in enumerate(batch):\n",
    "        imangle = np.asarray(Image.fromarray(im.squeeze()).rotate(25*np.random.randn()))\n",
    "        newbatch[i]=imangle\n",
    "    return newbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for batch_iter in xrange(total_iters):\n",
    "    # [X_train, Y_train] = iam_m.get_train_batch(batch_size*100)\n",
    "    print \"Getting batch number \"+str(batch_iter)+\".\"\n",
    "    (X_train, Y_train) = get_batch(author_hdf5_file, author_ids, data_size=32*100)\n",
    "    X_train = 1.0 - X_train / 255.0\n",
    "    X_train = np.expand_dims(X_train, 1)\n",
    "    X_train = randangle(X_train)\n",
    "    Y_train = to_categorical(Y_train, num_authors)\n",
    "    print \"Batch iteration \"+str(batch_iter)+\"/\"+str(total_iters)+\" on \"+str(num_authors)+\" authors.\"\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1, show_accuracy=True, verbose=1) #, validation_data=(X_test, Y_test))\n",
    "    # model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1, show_accuracy=True, verbose=1, validation_data=(X_train, Y_train))\n",
    "    if (batch_iter % 20)==0 and batch_iter != 0:\n",
    "        model.save_weights('fielnet-vtrain.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    (X_train, Y_train) = get_batch(author_hdf5_file, author_ids)\n",
    "    X_train = 1.0 - X_train / 255.0\n",
    "    X_train = np.expand_dims(X_train, 1)\n",
    "    X_train = randangle(X_train)\n",
    "\n",
    "plt.subplots(4,4)\n",
    "for i in xrange(16):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.imshow(X_train[i][0])\n",
    "print \"number of authors = \"+str(num_authors) + \", batch size = \"+str(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
