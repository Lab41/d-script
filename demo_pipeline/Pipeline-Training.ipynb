{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import sys\n",
    "import logging\n",
    "sys.path.append('../')\n",
    "\n",
    "# Neural network stuff\n",
    "import data_iters\n",
    "from data_iters.hdf5_iterator import Hdf5MiniBatcher\n",
    "from data_iters.minibatcher import MiniBatcher\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from fielutil import load_verbatimnet\n",
    "#from featextractor import extract_imfeats_debug\n",
    "\n",
    "from data_iters.archive.iam_iterator import IAM_MiniBatcher\n",
    "from data_iters.CoffeeStainer import CoffeeStainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File names\n",
    "\n",
    "You will require:\n",
    "1. HDF5 Files:\n",
    "    a. Author-Lines\n",
    "    b. Flat Images\n",
    "2. Params (for the neural network you're looking at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which training dataset do we want to train from?\n",
    "train_dataset='iam-lines'\n",
    "#train_dataset='nmec'\n",
    "\n",
    "\n",
    "# Do you want to load the features in? Or save them to a file?\n",
    "load_features = False\n",
    "\n",
    "# All the images that you require extraction should be in this HDF5 file\n",
    "if train_dataset=='nmec':\n",
    "    hdf5authors='/memory/nmec_scaled_author_form.hdf5'\n",
    "    hdf5authors='/fileserver/nmec-handwriting/author_nmec_bin_uint8.hdf5'\n",
    "    hdf5images='nmecdata/nmec_scaled_flat.hdf5'\n",
    "elif train_dataset=='iam-words':\n",
    "    hdf5authors='/fileserver/iam/iam-processed/words/author_words.hdf5'\n",
    "elif train_dataset=='iam-lines':\n",
    "    hdf5authors='/fileserver/iam/iam-processed/lines/author_lines.hdf5'\n",
    "else:\n",
    "    hdf5authors='/fileserver/iam/iam-processed/forms/author_forms.hdf5'\n",
    "\n",
    "# This is the file that you will load the features from or save the features to\n",
    "# featurefile = 'icdar13data/benchmarking-processed/icdar13be_fiel657.npy'\n",
    "# featurefile = 'icdar13data/experimental-processed/icdar13ex_fiel657.npy'\n",
    "featurefile = 'nmecdata/nmec_fiel657_features.npy'\n",
    "\n",
    "# This is the neural networks and parameters you are deciding to use\n",
    "paramsfile = '/fileserver/iam/iam-processed/models/fiel_657.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = h5py.File(hdf5authors, 'r')\n",
    "num_authors=len(labels)\n",
    "num_forms_per_author=-1\n",
    "shingle_dim=(56,56)\n",
    "batch_size=32\n",
    "load_size=batch_size*1000\n",
    "iterations = 1000\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your model\n",
    "\n",
    "Here, we're using the Fiel Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establishing Fiel's verbatim network\n",
      "Loaded neural network up to fc7 layer\n",
      "Finished compilation\n"
     ]
    }
   ],
   "source": [
    "vnet = load_verbatimnet( 'fc7', paramsfile=paramsfile, compiling=False )\n",
    "vnet.add(Dense(num_authors))\n",
    "vnet.add(Activation('softmax'))\n",
    "sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "vnet.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "print \"Finished compilation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatcher (to load in your data for each batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 099, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 2 being assigned zero documents for author 597, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 524, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 249, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 556, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 618, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 143, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 611, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 2 being assigned zero documents for author 611, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 615, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 2 being assigned zero documents for author 581, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 652, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 189, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 069, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 327, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 201, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 655, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 2 being assigned zero documents for author 669, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 653, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 543, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 604, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 572, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 258, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 502, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 2 being assigned zero documents for author 633, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 637, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 2 being assigned zero documents for author 462, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 075, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 268, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 535, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 538, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 115, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 426, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 309, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 307, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 1 being assigned zero documents for author 608, maybe increase minimum docs per author?\n",
      "WARNING:data_iters.minibatcher:Set 2 being assigned zero documents for author 479, maybe increase minimum docs per author?\n"
     ]
    }
   ],
   "source": [
    "# logging.getLogger('data_iters.hdf5_iterator').setLevel(logging.DEBUG)\n",
    "if True:\n",
    "    mini_m = Hdf5MiniBatcher(hdf5authors, num_authors, num_forms_per_author,\n",
    "                            shingle_dim=shingle_dim, default_mode=MiniBatcher.TRAIN,\n",
    "                            batch_size=load_size, postprocess=CoffeeStainer.nmec_post)\n",
    "else:\n",
    "    mini_m = IAM_MiniBatcher(hdf5authors, num_authors, num_forms_per_author,\n",
    "                            shingle_dim=shingle_dim, default_mode=MiniBatcher.TRAIN,\n",
    "                            batch_size=load_size, postprocess=CoffeeStainer.nmec_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model for however many specified iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 259s - loss: 15.5452 - acc: 0.0299   \n",
      "Finished training on the 0th batch\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 259s - loss: 15.2665 - acc: 0.0476   \n",
      "Finished training on the 1th batch\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 260s - loss: 15.2842 - acc: 0.0460   \n",
      "Finished training on the 2th batch\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 259s - loss: 15.2879 - acc: 0.0464   \n",
      "Finished training on the 3th batch\n",
      "Epoch 1/1\n",
      "32000/32000 [==============================] - 257s - loss: 15.2736 - acc: 0.0471   \n",
      "Finished training on the 4th batch\n",
      "Epoch 1/1\n",
      "21888/32000 [===================>..........] - ETA: 81s - loss: 15.2897 - acc: 0.0466"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a4547c45eb4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_authors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mvnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Finished training on the \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"th batch\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_iter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch_iter\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Keras-0.3.0-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, show_accuracy, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m    579\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m                          \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                          shuffle=shuffle, metrics=metrics)\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Keras-0.3.0-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, metrics)\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Keras-0.3.0-py2.7.egg/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# logging.getLogger('data_iters.hdf5_iterator').setLevel(logging.DEBUG)\n",
    "for batch_iter in range(iterations):\n",
    "    (X_train,Y_train) = mini_m.get_train_batch()\n",
    "    # X_train = 1.0 - X_train / 255.0\n",
    "    X_train = np.expand_dims(X_train, 1)\n",
    "    Y_train = to_categorical(Y_train, num_authors)\n",
    "    vnet.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1, show_accuracy=True, verbose=1)\n",
    "    print \"Finished training on the \"+str(batch_iter)+\"th batch\"\n",
    "    if (batch_iter % 20)==0 and batch_iter != 0:\n",
    "        vnet.save_weights('fielnet-nmec.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " vnet.fit(X_train, Y_train, batch_size=32, nb_epoch=1, show_accuracy=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
